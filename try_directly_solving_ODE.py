import inner_optimisation_scipy
from inner_optimisation_scipy import *
# from generate_data import *
import pints_classes
from pints_classes import *
import multiprocessing as mp
import pandas as pd
from itertools import repeat
import csv
import os
import pickle as pkl

# definitions
# set up variables for the simulation
tlim = [300, 14899]
times = np.linspace(*tlim, tlim[-1] - tlim[0], endpoint=False)
voltage = V(times)  # must read voltage at the correct times to match the output
del tlim
model_name = 'Kemp' # this is the generative model name, can be HH or Kemp
snr_db = 20 # signal to noise ratio in dB
## set up the parameters for the fitted model
fitted_model = hh_model
state_names = ['a', 'r'] # how many states we have in the model that we are fitting
## settings for the inner optimisation
upper_bound_beta = 0.9999 # upper bound for the betas
lambd = 10e5  # gradient matching weight - test
# outer optimisation settings
inLogScale = True  # is the search of thetas in log scale
convergence_threshold = 1e-8
iter_for_convergence = 1000
max_iter_outer = 1000
## rectangular boundaries of thetas from Clerx et.al. paper - they are the same for two gating variables + one for conductance
theta_lower_boundary = [np.log(10e-5), np.log(10e-5), np.log(10e-5), np.log(10e-5), np.log(10e-5), np.log(10e-5),
                        np.log(10e-5), np.log(10e-5), np.log(10e-3), np.log(10e-10),np.log(10e-10)]
theta_upper_boundary = [np.log(10e3), np.log(0.4), np.log(10e3), np.log(0.4), np.log(10e3), np.log(0.4),
                        np.log(10e3), np.log(0.4), np.log(10), 0, 0]
# parallelisation settings
ncpu = mp.cpu_count()
ncores = 12
####################################################################################################################
### from this point no user changes are required
####################################################################################################################
# load the protocols
load_protocols
# generate the segments with B-spline knots and intialise the betas for  - we won't actully need knots for the optimisation
jump_indeces, times_roi, voltage_roi, knots_roi, collocation_roi, spline_order = generate_knots(times)
jumps_odd = jump_indeces[0::2]
jumps_even = jump_indeces[1::2]
print('The time axis is split into ' + str(nSegments) + ' segments based on protocol steps.')

def run_ode(Thetas_ODE, times, fitted_model):
    thetas = Thetas_ODE[:-2]
    init_conds = Thetas_ODE[-2:]
    solution = sp.integrate.solve_ivp(fitted_model, [0, times[-1]], init_conds, args=[thetas],
                                                dense_output=True, method='LSODA', rtol=1e-8, atol=1e-8)
    states = solution.sol(times)
    derivs = np.gradient(states, times, axis=1)
    rhs = np.array(fitted_model(times, states, thetas))
    return (states, derivs, rhs)


# main
if __name__ == '__main__':
    debug_opt = False
    # generate synthetic data
    if model_name.lower() not in available_models:
        raise ValueError(f'Unknown model name: {model_name}. Available models are: {available_models}')
    elif model_name.lower() == 'hh':
        thetas_true = thetas_hh_baseline
    elif model_name.lower() == 'kemp':
        thetas_true = thetas_kemp  # the last parameter is the conductance - get it as a separate variable just in case
    elif model_name.lower() == 'wang':
        thetas_true = thetas_wang  # the last parameter is the conductance
    Thetas_ODE = thetas_true
    param_names = [f'p_{i}' for i in range(1, len(Thetas_ODE) + 1)]
    solution, current_model = generate_synthetic_data(model_name, thetas_true, times)
    states_true = solution.sol(times) # get the true states generated by the model
    snr = 10 ** (snr_db / 10) # convert the signal to noise ratio to linear scale
    current_true = current_model(times, solution, thetas_true, snr=snr) # get the true current
    print('Produced synthetic data for the ' + model_name + ' model based on the pre-loaded voltage protocol.')
    ####################################################################################################################
    # set up table for saving results
    # gen_model means generative model - we are denoting which model we are using to generate the data
    folderName = 'Results_gen_model_' + model_name + '_IVP'
    if not os.path.exists(folderName):
        os.makedirs(folderName)
    # save the data to a pickle file
    with open(folderName +'/synthetic_data.pkl', 'wb') as f:
        pkl.dump([times, voltage, current_true, states_true, thetas_true, knots_roi, snr_db], f)
    ####################################################################################################################
    # set up boundaries for thetas and initialise depending on the scale of search
    if inLogScale:
        # theta in log scale
        thetas_with_ic = thetas_hh_baseline + [0.5, 0.5]
        init_thetas = np.log(thetas_with_ic)  # start around the true solution to see how long it takes to converge
        sigma0_thetas = 0.1 * np.ones_like(init_thetas)
        # boundaries_thetas = pints.RectangularBoundaries(theta_lower_boundary, theta_upper_boundary) # rectangular boundaries
        boundaries_thetas = BoundariesTwoStatesWithInit()  # boundaries accounting for the reaction rates
    else:
        # theta in decimal scale
        thetas_with_ic = thetas_hh_baseline + [0.5, 0.5]
        init_thetas = 0.001 * np.ones_like(thetas_hh_baseline)
        sigma0_thetas = 0.0005 * np.ones_like(init_thetas)
        boundaries_thetas = pints.RectangularBoundaries(np.exp(theta_lower_boundary), np.exp(theta_upper_boundary))
    Thetas_ODE = init_thetas.copy()
    ####################################################################################################################
    ## create pints objects for the outer optimisation
    model_segments = SegmentOutput()
    ## create the problem of comparing the modelled current with measured curren
    values_to_match_output= np.transpose(np.array([current_true, voltage]))
    # ^ we actually only need first two columns in this array but pints wants to have the same number of values and outputs
    problem_outer = pints.MultiOutputProblem(model=model_segments, times=times,
                                             values=values_to_match_output)
    ## associate the cost with it
    error_outer = OuterCriterion(problem=problem_outer)
    error_outer_no_model = OuterCriterionNoModel(problem=problem_outer) # test if running without model is faster
    ## create the optimiser
    optimiser_outer = pints.CMAES(x0=init_thetas, sigma0=sigma0_thetas,boundaries=boundaries_thetas)
    optimiser_outer.set_population_size(min(len(Thetas_ODE) * 7, 30)) # restrict the population size to 30
    ####################################################################################################################
    # take 1: loosely based on ask-tell example from  pints
    ## Run optimisation
    theta_visited = []
    theta_guessed = []
    f_guessed = []
    theta_best = []
    f_outer_best = []
    f_gradient_best = []
    OuterCosts_all = []
    GradientCosts_all = []
    # create a logger file
    csv_file_name = folderName + '/iterations_both_states.csv'
    theta_names = ['Theta_' + str(i) for i in range(len(Thetas_ODE))] # create names for all thetas
    column_names = ['Iteration', 'Walker'] +  theta_names +  ['Outer Cost']
    ####################################################################################################################
    # open the file to write to
    big_tic = tm.time()
    with open(csv_file_name, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(column_names)
        # run the outer optimisation
        for i in range(max_iter_outer):
            # get the next points (multiple locations)
            thetas = optimiser_outer.ask()
            # deal with the log scale
            if inLogScale:
                # convert thetas to decimal scale for inner optimisation
                thetasForInner = np.exp(thetas)
            else:
                thetasForInner = thetas
            # create the placeholder for cost functions
            OuterCosts = []
            GradientCosts = []
            # for each theta in the sample
            tic = tm.time()
            # run inner optimisation for each theta sample
            with mp.get_context('fork').Pool(processes=min(ncpu, ncores)) as pool:
                results = pool.starmap(run_ode,zip(thetasForInner, repeat(times), repeat(fitted_model)))
            # package results is a list of tuples
            # extract the results
            for iSample, result in enumerate(results):
                states_at_sample, derivs_at_sample, rhs_at_sample = result
                # get the states at this sample
                state_all_segments = np.array(states_at_sample)
                pints_classes.state_all_segments = state_all_segments # send the variable into pint classes
                # store the costs
                OuterCost = error_outer(thetasForInner[iSample, :])
                d_deriv = (derivs_at_sample - rhs_at_sample) ** 2
                axis_of_time = np.argmax(d_deriv.shape)
                integral_quad = sp.integrate.simpson(y=d_deriv, even='avg', axis=axis_of_time)
                GradCost = np.sum(integral_quad, axis=0)
                OuterCosts.append(OuterCost)
                GradientCosts.append(GradCost)
            # tell the optimiser about the costs
            optimiser_outer.tell(OuterCosts)
            # store the best point
            index_best = OuterCosts.index(min(OuterCosts))
            theta_best.append(thetas[index_best, :])
            f_outer_best.append(OuterCosts[index_best])
            f_gradient_best.append(GradientCosts[index_best])
            OuterCosts_all.append(OuterCosts)
            GradientCosts_all.append(GradientCosts)
            # store the visited points
            theta_visited.append(thetas)
            # print the results
            print('Iteration: ', i+1)
            print('Best parameters: ', theta_best[-1])
            print('Best objective: ', f_outer_best[-1])
            print('Mean objective: ', np.mean(OuterCosts))
            print('Gradient objective at best sample: ', f_gradient_best[-1])
            print('Time elapsed: ', tm.time() - tic)
            # write the results to a csv file
            for iWalker in range(len(thetas)):
                row = [i, iWalker] + list(thetas[iWalker]) + [OuterCosts[iWalker]]
                writer.writerow(row)
            file.flush()

            # check for convergence
            if (i > iter_for_convergence):
                # check how the cost increment changed over the last 10 iterations
                d_cost = np.diff(f_outer_best[-iter_for_convergence:])
                # if all incrementa are below a threshold break the loop
                if all(d <= convergence_threshold for d in d_cost):
                    print("No changes in" + str(iter_for_convergence) + "iterations. Terminating")
                    break
            ## end convergence check condition
        ## end loop over iterations
    ## close the cvs file into which we were writing the results
    big_toc = tm.time()
    # convert the lists to numpy arrays
    theta_best = np.array(theta_best)
    f_outer_best = np.array(f_outer_best)
    f_gradient_best = np.array(f_gradient_best)
    print('Total time taken: ', big_toc - big_tic)
    print('===========================================================================================================')
    ####################################################################################################################
    ## simulate the optimised model using B-splines
    Thetas_ODE = theta_best[-1]
    if inLogScale:
        # convert thetas to decimal scale for inner optimisation
        Thetas_ODE = np.exp(Thetas_ODE)
    else:
        Thetas_ODE = Thetas_ODE
    ## simulate the model using the best thetas and the ODE model used
    states_optimised_ODE, deriv_optimised_ODE, RHS_optimised_ODE = run_ode(Thetas_ODE, times,fitted_model)
    current_optimised_ODE = observation_direct_input(states_optimised_ODE, voltage, Thetas_ODE)
    ####################################################################################################################
    ## create figures so you can populate them with the data
    fig, axes = plt.subplot_mosaic([['a)'], ['b)'], ['c)']], layout='constrained', sharex=True)
    y_labels = ['I'] + state_names
    for _, ax in axes.items():
        for iSegment, SegmentStart in enumerate(jumps_odd):
            ax.axvspan(times[SegmentStart], times[jumps_even[iSegment]], facecolor='0.2', alpha=0.1)
    axes['a)'].plot(times, current_true, '-k', label=r'Current true', linewidth=1.5, alpha=0.5)
    # add test to plots - compare the modelled current with the true current
    axes['a)'].plot(times, current_optimised_ODE, '-m',
                    label=r'Current from ODE solution', linewidth=1, alpha=0.7)
    axes['b)'].plot(times, states_optimised_ODE[0, :], '-m',
                    label=r'Fitted ODE solution using IVP',
                    linewidth=1, alpha=0.7)
    axes['c)'].plot(times, states_optimised_ODE[1, :], '-m',
                    label=r'Fitted ODE solution using IVP',
                    linewidth=1, alpha=0.7)
    iAx = 0
    for _, ax in axes.items():
        ax.set_ylabel(y_labels[iAx], fontsize=12)
        ax.set_facecolor('white')
        ax.grid(which='major', color='grey', linestyle='solid', alpha=0.2, linewidth=1)
        ax.legend(fontsize=12, loc='best')
        iAx += 1
    # plt.tight_layout(pad=0.3)
    fig.savefig(folderName + '/states_model_output.png', dpi=400)
    ####################################################################################################################
    fig1, axes1 = plt.subplot_mosaic([['a)'], ['b)'], ['c)']], layout='constrained', sharex=True)
    y_labels1 = ['I_{true} - I_{model}', 'da - RHS', 'dr - RHS']
    for _, ax in axes1.items():
        for iSegment, SegmentStart in enumerate(jumps_odd):
            ax.axvspan(times[SegmentStart], times[jumps_even[iSegment]], facecolor='0.2', alpha=0.1)
    axes1['a)'].plot(times, current_true - current_optimised_ODE, '--k',
                     label=r'Current from ODE solution', linewidth=1, alpha=0.7)
    axes1['b)'].plot(times, deriv_optimised_ODE[0,:] - RHS_optimised_ODE[0,:],
                     '--k', label=r'Gradient matching error using IVP', linewidth=1, alpha=0.7)
    axes1['c)'].plot(times, deriv_optimised_ODE[1,:] - RHS_optimised_ODE[1,:],
                     '--k', label=r'Gradient matching using IVP', linewidth=1, alpha=0.7)
    ## save the figures
    iAx = 0
    for _, ax in axes1.items():
        ax.set_ylabel(y_labels1[iAx], fontsize=12)
        ax.set_facecolor('white')
        ax.grid(which='major', color='grey', linestyle='solid', alpha=0.2, linewidth=1)
        ax.legend(fontsize=12, loc='best')
        iAx += 1
    # plt.tight_layout(pad=0.3)
    fig1.savefig(folderName + '/errors_model_output.png', dpi=400)
    ####################################################################################################################

    # plot evolution of outer costs
    plt.figure(figsize=(10, 6))
    plt.semilogy()
    plt.xlabel('Iteration')
    plt.ylabel('Outer optimisation cost')
    for iIter in range(len(f_outer_best) - 1):
        plt.scatter(iIter * np.ones(len(OuterCosts_all[iIter])), OuterCosts_all[iIter], c='k', marker='.', alpha=.5,
                    linewidths=0)
    iIter += 1
    plt.scatter(iIter * np.ones(len(OuterCosts_all[iIter])), OuterCosts_all[iIter], c='k', marker='.', alpha=.5,
                linewidths=0, label='Sample cost: H(Theta / C, Y)')
    plt.plot(f_outer_best, '-b', linewidth=1.5,
             label='Best cost:H(Theta_{best} / C, Y) = ' + "{:.5e}".format(f_outer_best[-1]))
    plt.legend(loc='best')
    plt.tight_layout()
    plt.savefig(folderName + '/outer_cost_evolution.png', dpi=400)

    # plot evolution of outer costs
    plt.figure(figsize=(10, 6))
    plt.semilogy()
    plt.xlabel('Iteration')
    plt.ylabel('Gradient matching cost')
    for iIter in range(len(f_gradient_best) - 1):
        plt.scatter(iIter * np.ones(len(GradientCosts_all[iIter])), GradientCosts_all[iIter], c='k', marker='.', alpha=.5,
                    linewidths=0)
    iIter += 1
    plt.scatter(iIter * np.ones(len(GradientCosts_all[iIter])), GradientCosts_all[iIter], c='k', marker='.', alpha=.5,
                linewidths=0, label='Sample cost: G_{ODE}(Theta, Y)')
    # plt.plot(range(len(f_gradient_best)), np.ones(len(f_gradient_best)) * GradCost_given_true_theta, '--m', linewidth=2.5, alpha=.5,label='Collocation solution: G_{ODE}( C /  Theta_{true}, Y) = ' + "{:.5e}".format(
    #              GradCost_given_true_theta))
    plt.plot(f_gradient_best, '-b', linewidth=1.5,
             label='Best cost:G_{ODE}(C / Theta, Y) = ' + "{:.5e}".format(f_gradient_best[-1]))
    plt.legend(loc='best')
    plt.tight_layout()
    plt.savefig(folderName + '/gradient_cost_evolution.png', dpi=400)


    # plot parameter values after search was done on decimal scale
    if inLogScale:
        fig, axes = plt.subplots(len(Thetas_ODE)-2, 1, figsize=(3 * (len(Thetas_ODE)-2), 16), sharex=True)
        for iAx, ax in enumerate(axes.flatten()):
            for iIter in range(len(theta_best)):
                x_visited_iter = theta_visited[iIter][:, iAx]
                ax.scatter(iIter * np.ones(len(x_visited_iter)), x_visited_iter, c='k', marker='.', alpha=.2, linewidth=0)
            # ax.plot(range(iIter+1),np.ones(iIter+1)*theta_true[iAx], '--m', linewidth=2.5,alpha=.5, label=r"true: log("+param_names[iAx]+") = " +"{:.6f}".format(theta_true[iAx]))
            # ax.plot(theta_guessed[:,iAx],'--r',linewidth=1.5,label=r"guessed: $\theta_{"+str(iAx+1)+"} = $" +"{:.4f}".format(theta_guessed[-1,iAx]))
            ax.plot(theta_best[:, iAx], '-m', linewidth=1.5,
                    label=r"best: log(" + param_names[iAx] + ") = " + "{:.6f}".format(theta_best[-1, iAx]))
            ax.set_ylabel('log(' + param_names[iAx] + ')')
            ax.legend(loc='best')
        plt.tight_layout()
        plt.savefig(folderName + '/ODE_params_log_scale.png', dpi=400)
        # separate for ICs
        fig, axes = plt.subplots(2, 1, figsize=(3 * 2, 16), sharex=True)
        for iAx, ax in enumerate(axes.flatten()):
            iAx += len(Thetas_ODE) - 2
            for iIter in range(len(theta_best)):
                x_visited_iter = theta_visited[iIter][:, iAx]
                ax.scatter(iIter * np.ones(len(x_visited_iter)), x_visited_iter, c='k', marker='.', alpha=.2,
                           linewidth=0)
            ax.plot(theta_best[:, iAx], '-m', linewidth=1.5,
                    label=r"best: log(" + param_names[iAx] + ") = " + "{:.6f}".format(theta_best[-1, iAx]))
            ax.set_ylabel('log(' + param_names[iAx] + ')')
            ax.legend(loc='best')
        plt.tight_layout()
        plt.savefig(folderName + '/ICs_log_scale.png', dpi=400)
        # plot parameter values converting from log scale to decimal
        fig, axes = plt.subplots(len(Thetas_ODE) - 2, 1, figsize=(3 * (len(Thetas_ODE) - 2), 16), sharex=True)
        for iAx, ax in enumerate(axes.flatten()):
            for iIter in range(len(theta_best)):
                x_visited_iter = theta_visited[iIter][:, iAx]
                ax.scatter(iIter * np.ones(len(x_visited_iter)), np.exp(x_visited_iter), c='k', marker='.', alpha=.2,
                           linewidth=0)
            ax.plot(np.exp(theta_best[:, iAx]), '-m', linewidth=1.5,
                    label=r"best: " + param_names[iAx] + " = " + "{:.6f}".format(np.exp(theta_best[-1, iAx])))
            ax.set_ylabel('log(' + param_names[iAx] + ')')
            ax.legend(loc='best')
        plt.tight_layout()
        plt.savefig(folderName + '/ODE_params_decimal.png', dpi=400)
        # separate for ICs
        fig, axes = plt.subplots(2, 1, figsize=(3 * 2, 16), sharex=True)
        for iAx, ax in enumerate(axes.flatten()):
            iAx += len(Thetas_ODE) - 2
            for iIter in range(len(theta_best)):
                x_visited_iter = theta_visited[iIter][:, iAx]
                ax.scatter(iIter * np.ones(len(x_visited_iter)), np.exp(x_visited_iter), c='k', marker='.', alpha=.2,
                           linewidth=0)
            ax.plot(np.exp(theta_best[:, iAx]), '-m', linewidth=1.5,
                    label=r"best: log(" + param_names[iAx] + ") = " + "{:.6f}".format(np.exp(theta_best[-1, iAx])))
            ax.set_ylabel('log(' + param_names[iAx] + ')')
            ax.legend(loc='best')
        plt.tight_layout()
        plt.savefig(folderName + '/ICs_decimal.png', dpi=400)
    else:
        fig, axes = plt.subplots(len(Thetas_ODE) - 2, 1, figsize=(3 * (len(Thetas_ODE) - 2), 16), sharex=True)
        for iAx, ax in enumerate(axes.flatten()):
            for iIter in range(len(theta_best)):
                x_visited_iter = theta_visited[iIter][:, iAx]
                ax.scatter(iIter * np.ones(len(x_visited_iter)), x_visited_iter, c='k', marker='.', alpha=.2,
                           linewidth=0)
            ax.plot(theta_best[:, iAx], '-m', linewidth=1.5,
                    label=r"best: " + param_names[iAx] + " = " + "{:.6f}".format(theta_best[-1, iAx]))
            ax.set_ylabel('log(' + param_names[iAx] + ')')
            ax.legend(loc='best')
        plt.tight_layout()
        plt.savefig(folderName + '/ODE_params_decimal.png', dpi=400)
        # separate for ICs
        fig, axes = plt.subplots(2, 1, figsize=(3 * 2, 16), sharex=True)
        for iAx, ax in enumerate(axes.flatten()):
            iAx += len(Thetas_ODE) - 2
            for iIter in range(len(theta_best)):
                x_visited_iter = theta_visited[iIter][:, iAx]
                ax.scatter(iIter * np.ones(len(x_visited_iter)), x_visited_iter, c='k', marker='.', alpha=.2,
                           linewidth=0)
            ax.plot(theta_best[:, iAx], '-m', linewidth=1.5,
                    label=r"best: log(" + param_names[iAx] + ") = " + "{:.6f}".format(theta_best[-1, iAx]))
            ax.set_ylabel('log(' + param_names[iAx] + ')')
            ax.legend(loc='best')
        plt.tight_layout()
        plt.savefig(folderName + '/ICs_decimal.png', dpi=400)